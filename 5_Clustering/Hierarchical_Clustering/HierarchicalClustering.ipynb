{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hierarchial_Clustering_Writeup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU22Gz_V2S5T"
      },
      "source": [
        "# ***Hierarchial Clustering***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocTYyRnW2dEr"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZbooWR2LkS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtuE_-5I2lS5"
      },
      "source": [
        "### Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A03wI6l22n_E"
      },
      "source": [
        "dataset = pd.read_csv('Mall_Customers.csv') #loading/reading dataset using pandas\n",
        "X = dataset.iloc[:, [3, 4]].values #selecting all values of indexes 3 and 4 which are annual income and spending score\n",
        "#y = dataset.iloc[:, 3].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwdY7Mih2s0S"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhxVaiJ22tp0"
      },
      "source": [
        "'''from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UKSddZ72zvD"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQTHnBVw2zMX"
      },
      "source": [
        "\"\"\"from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "sc_y = StandardScaler()\n",
        "y_train = sc_y.fit_transform(y_train)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56z1sbFK275A"
      },
      "source": [
        "### Using the dendrogram to find the optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-CWFBfm259g"
      },
      "source": [
        "import scipy.cluster.hierarchy as sch   #importing cluster.hierarchy from scipy as sch\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))  #using dendogram function and inputting linkage function as its argument. Method ward is used for minimizing variance inside clusters\n",
        "plt.title('Dendrogram') #Naming Title of graph\n",
        "plt.xlabel('Customers') #Naming entity being represented on x axis\n",
        "plt.ylabel('Euclidean distances') #Naming entity being represented on y axis\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UGk5jkw2_6Y"
      },
      "source": [
        "### Fitting Hierarchical Clustering to the datase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4KKSvB3Crn"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering  \n",
        "hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward') #hc is an object of AgglomerativeClustering. We specify Number of clusters as 5 based on dendograms, affinity is the distance method you want to use. \n",
        "y_hc = hc.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39V2-09J3FKo"
      },
      "source": [
        "### Visualising the clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOXNyysQ3IL3"
      },
      "source": [
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
        "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
        "plt.title('Clusters of customers')\n",
        "plt.xlabel('Annual Income (k$)')\n",
        "plt.ylabel('Spending Score (1-100)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
